{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='logo.png' align='center'>\n",
    "<div align='center' class=\"h2\">深度学习理论与实践编程练习(Course 02)</div>\n",
    "\n",
    "-----\n",
    "### 命名格式：按照课程网站中的课后作业要求\n",
    "-----\n",
    "\n",
    "#### 1. 根据Course02课程中对全连接神经网络的讲解，将缺失的全连接神经网络中代码块补全，并完成一次训练\n",
    "    需要填充的部分已经在第一部分的全连接神经网络代码中用红色的\"\"\"补全代码\"\"\"标示\n",
    "    \n",
    "    \n",
    "#### 2. 利用sklearn的指标评测函数得出模型在测试集上的性能\n",
    "    从sklearn官网上查找metrics子包中关于评价指标的计算函数，了解其用法，之后的课程中就用到。\n",
    "-----\n",
    "\n",
    "【目录】\n",
    "\n",
    "<a href=\"#1.-手写数字识别数据集(MNIST)\">1. 手写数字识别数据集（MNIST）</a>\n",
    "\n",
    "<a href=\"#2.-全连接神经网络(Full-Connected-Neural-Network)\" >2. 全连接神经网络</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "## 1. 手写数字识别数据集(MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 引入python包，没有安装的请按照抛出的error通过conda来安装直至成功\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import sklearn\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  载入mnist数据\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('MNIST/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = cPickle.load(f, encoding='bytes')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def minist_loader():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    # cause the network's input is a vector 784X1, so we reshape the image so as to more convenient\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = list(zip(training_inputs, training_results))\n",
    "\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = list(zip(validation_inputs, va_d[1]))\n",
    "\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = list(zip(test_inputs, te_d[1]))\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = minist_loader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data[0][0]))\n",
    "print(training_data[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=5, sharex='all', sharey='all')\n",
    "ax = ax.flatten()\n",
    "for i in range(25):\n",
    "    ax[i].set_title(int(test_data[i][1]))\n",
    "    img = test_data[i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"训练集个数：{} 验证集个数：{} 测试集个数：{}\".format(len(training_data), len(validation_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "----\n",
    "\n",
    "## 2. 全连接神经网络(Full Connected Neural Network)\n",
    "全连神经网络从前向后分为`输入层` -> `隐藏层`xM -> `输出层`。如果网络共有N层的话，那么存在N-1个权重连接矩阵和N-1个对应的偏置向量，其维度分别为:\n",
    "\n",
    "$$\n",
    "\\large\n",
    "W^{l_i} \\in R^{l_i \\times l_{i-1}} \\quad (1<l_i \\leq N)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\large\n",
    "b^{l_i} \\in R^{l_i } \\quad (1 < l_i \\leq N) \n",
    "$$\n",
    "\n",
    "<!-- <img src=\"FCNN.png\" width=\"40%\" align=\"center\"> -->\n",
    "\n",
    "\n",
    "全连接神经网络的构建分为前向传播和误差的反向传播，具体的过程如下图：\n",
    "<img src='BP.png' align='center'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid激活函数定义\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"补全代码\"\"\"\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"\n",
    "    Sigmoid函数的导数,关于Sigmoid函数的求导可以自行搜索。\n",
    "    \"\"\"\n",
    "    \"\"\"补全代码\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Connected Network\n",
    "class FCN(object):\n",
    "    \"\"\"\n",
    "    全连接网络的纯手工实现\n",
    "    \"\"\"\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"\n",
    "        :param sizes: 是一个列表，其中包含了神经网络每一层的神经元的个数，列表的长度就是神经网络的层数。\n",
    "        举个例子，假如列表为[784,30,10]，那么意味着它是一个3层的神经网络，第一层包含784个神经元，第二层30个，最后一层10个。\n",
    "        注意，神经网络的权重和偏置是随机生成的，使用一个均值为0，方差为1的高斯分布。\n",
    "        注意第一层被认为是输入层，它是没有偏置向量b和权重向量w的。因为偏置只是用来计算第二层之后的输出\n",
    "        \"\"\"\n",
    "        self._num_layers = len(sizes) # 记录神经网络的层数\n",
    "        # 为隐藏层和输出层生成偏置向量b，还是以[784,30,10]为例，那么一共会生成2个偏置向量b，分别属于隐藏层和输出层，大小分别为30x1,10x1。\n",
    "        \"\"\"补全代码\"\"\"\n",
    "        # 为隐藏层和输出层生成权重向量W, 以[784,30,10]为例，这里会生成2个权重向量w，分别属于隐藏层和输出层，大小分别是30x784, 10x30。\n",
    "        \"\"\"补全代码\"\"\"\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        前向计算，返回神经网络的输出。公式如下:\n",
    "        output = sigmoid(w*x+b)\n",
    "        以[784,30,10]为例，权重向量大小分别为[30x784, 10x30]，偏置向量大小分别为[30x1, 10x1]\n",
    "        输入向量为 784x1.\n",
    "        矩阵的计算过程为：\n",
    "            30x784 * 784x1 = 30x1\n",
    "            30x1 + 30x1 = 30x1\n",
    "\n",
    "            10x30 * 30x1 = 10x1\n",
    "            10x1 + 10x1 = 10x1\n",
    "            故最后的输出是10x1的向量，即代表了10个数字。\n",
    "        :param a: 神经网络的输入\n",
    "        \"\"\"\n",
    "        for w, b in zip(self._weights, self._biases):\n",
    "            \"\"\"补全代码\"\"\"\n",
    "        return a\n",
    "\n",
    "    def train(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        使用小批量随机梯度下降来训练网络\n",
    "        :param training_data: training data 是一个元素为(x, y)元祖形式的列表，代表了训练数据的输入和输出。\n",
    "        :param epochs: 训练轮次\n",
    "        :param mini_batch_size: 小批量训练样本数据集大小\n",
    "        :param eta: 学习率\n",
    "        :param test_data: 如果test_data被指定，那么在每一轮迭代完成之后，都对测试数据集进行评估，计算有多少样本被正确识别了。但是这会拖慢训练速度。\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if test_data: \n",
    "            n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            # 在每一次迭代之前，都将训练数据集进行随机打乱，然后每次随机选取若干个小批量训练数据集\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "\n",
    "            # 每次训练迭代周期中要使用完全部的小批量训练数据集\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "\n",
    "            # 如果test_data被指定，那么在每一轮迭代完成之后，都对测试数据集进行评估，计算有多少样本被正确识别了\n",
    "            if test_data:\n",
    "                print(\"Epoch %d: accuracy rate: %.2f%%\" % (j, self.evaluate(test_data)/n_test*100))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        通过小批量随机梯度下降以及反向传播来更新神经网络的权重和偏置向量\n",
    "        :param mini_batch: 随机选择的小批量\n",
    "        :param eta: 学习率\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self._biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self._weights]\n",
    "        for x,y in mini_batch:\n",
    "            # 反向传播算法，运用链式法则求得对b和w的偏导\n",
    "            \"\"\"补全代码\"\"\"\n",
    "            \n",
    "            # 对小批量训练数据集中的每一个求得的偏导数进行累加\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "        # 使用梯度下降得出的规则来更新权重和偏置向量\n",
    "        self._weights = [w - (eta / len(mini_batch)) * nw\n",
    "                        for w, nw in zip(self._weights, nabla_w)]\n",
    "        self._biases = [b - (eta / len(mini_batch)) * nb\n",
    "                       for b, nb in zip(self._biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        反向传播算法，计算损失对w和b的梯度\n",
    "        :param x: 训练数据x\n",
    "        :param y: 训练数据x对应的标签\n",
    "        :return: Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "                gradient for the cost function C_x.  ``nabla_b`` and\n",
    "                ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "                to ``self.biases`` and ``self.weights``.\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self._biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self._weights]\n",
    "        # 前向传播，计算网络的输出\n",
    "        activation = x\n",
    "        # 一层一层存储全部激活值的列表\n",
    "        activations = [x]\n",
    "        # 一层一层存储全部的z向量，即带权输入\n",
    "        zs = []\n",
    "        for b, w in zip(self._biases, self._weights):\n",
    "            # 利用 z = wt*x+b 依次计算网络的输出\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            # 将每个神经元的输出z通过激活函数sigmoid\n",
    "            activation = sigmoid(z)\n",
    "            # 将激活值放入列表中暂存\n",
    "            activations.append(activation)\n",
    "        \n",
    "        # 反向传播过程\n",
    "\n",
    "        # 首先计算输出层的误差delta L\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        # 反向存储 损失函数C对b的偏导数\n",
    "        nabla_b[-1] = delta\n",
    "        # 反向存储 损失函数C对w的偏导数\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # 从第二层开始，依次计算每一层的神经元的偏导数\n",
    "        for l in range(2, self._num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            # 更新得到前一层的误差delta\n",
    "            \"\"\"补全代码\"\"\"\n",
    "            \n",
    "            # 保存损失喊出C对b的偏导数，它就等于误差delta\n",
    "            nabla_b[-l] = delta\n",
    "            # 计算损失函数C对w的偏导数\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l - 1].transpose())\n",
    "        # 返回每一层神经元的对b和w的偏导数\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        返回神经网络对测试数据test_data的预测结果，并且计算其中识别正确的个数\n",
    "        因为神经网络的输出是一个10x1的向量，我们需要知道哪一个神经元被激活的程度最大，\n",
    "        因此使用了argmax函数以获取激活值最大的神经元的下标，那就是网络输出的最终结果。\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        返回损失函数对a的的偏导数，损失函数定义 C = 1/2*||y(x)-a||^2\n",
    "        求导的结果为：\n",
    "            C' = y(x) - a\n",
    "        \"\"\"\n",
    "        return (output_activations - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个3层全连接网络，输入层有784个神经元，隐藏层30个神经元，输出层10个神经元\n",
    "fc = FCN([784, 30,10，10])\n",
    "# 设置迭代次数40次，mini-batch大小为10，学习率为3，并且设置测试集，即每一轮训练完成之后，都对模型进行一次评估。\n",
    "# 这里的参数可以根据实际情况进行修改\n",
    "fc.train(training_data, 5, 128, 5.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 课后作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业1选择题答案: B, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业2:\n",
    "* 2.1尝试推导逻辑回归的损失函数表达式:\n",
    "$J(w) = −\\sum y_i ln p(x_i) + (1 − y_i)ln(1 − p(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解:\n",
    "对于一个多元线性函数$h(x)=w^Tx_i$,将h(x)的值压缩到范围(0,1)使用sigmoid概率函数:$p(z)={1\\over 1+e^{-z}}$\n",
    "\n",
    "$p(x)={1\\over 1+e^{-(w^Tx_i)}}$\n",
    "\n",
    "设:$p(y=1|x)=p(x)$\n",
    "\n",
    "则:$p(y=0|x)=1-p(x)$\n",
    "\n",
    "所以将等式合并成一个就可以写成: $p(x)=p(x_i)^{y_i}\\cdot{[1-p(x_i)]}^{1-y_i}$\n",
    "\n",
    "那么所有样本的似然函数为:$\\prod{p(x_i)^{y_i}[1-p(x_i)]}^{(1-y_i)}$\n",
    "\n",
    "对函数两边求对数:$\\sum{y_ilnp(x_i)+(i-y_i)ln(1-p(x_i))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(w) = \\sum{y_ilnp(x_i)+(i-y_i)ln(1-p(x_i))}$\n",
    "\n",
    "$J(w) = -L$\n",
    "\n",
    "$J(w) = -\\sum{y_ilnp(x_i)+(i-y_i)ln(1-p(x_i))}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.2 请证明:为什么负梯度方向是损失函数下降的方向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解:\n",
    "对于损失函数$J(w^{k+1})=J(w^k+\\Delta x)$\n",
    "\n",
    "函数$J(w^{k+1})$参考$w^k$点的一阶泰勒展开式为:\n",
    "\n",
    "根据泰勒公式,$f(x)$在$x_0$处的泰勒公式为: $$\\sum_{n=0}^\\infty {f^{(n)}(x_0)\\over n!}(x-x_0)$$\n",
    "\n",
    "那么$J(w^{k+1})$在$w^k$处的一阶泰勒展开为 $J(w^{k+1}) \\approx J(w^k) + J'(w^k)(w^{k+1}-w)$\n",
    "\n",
    "将函数$J(w^k)$换到等号的左边:$J(w^{k+1}) - J(w^k) \\approx J'(w^k)(w^{k+1}-w)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了保证损失函数下降 $J(w^{k+1}) - J(w^k) < 0$\n",
    "\n",
    "$J'(w^k)(w^{k+1}-w) < 0$\n",
    "\n",
    "当$J'(w^k)>0$时, $(w^{k+1}-w)<0$\n",
    "\n",
    "当$J'(w^k)<0$时, $(w^{k+1}-w)>0$\n",
    "\n",
    "所以当损失函数下降,$(w^{k+1}-w)<0$和梯度的方向是相反的,并且完全相反时,下降是最快的,即负梯度的方向就是损失函数下降的方向"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业3:\n",
    "* 现有如下训练数据集,正样本点$M_1(3,3)$和$M_2(4,3)$,负样本点$M_3(1,1)$ ,请利用感知机算法求感知机模型$f(x) = sign(w^Tx + b)$。下方表格中已给出了前4步迭代的步骤,第5步迭代中选择M_2 为误分类点,请补全后续详细的迭代过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = torch.tensor([(3,3),(1,1),(4,3)])\n",
    "lable = torch.tensor([1, -1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = lambda x: 1 if x > 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return (x * w).sum()+ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor([0,0])\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:0, w:tensor([3, 3]), b:1\n",
      "idx:1, w:tensor([2, 2]), b:0\n",
      "idx:1, w:tensor([1, 1]), b:-1\n",
      "idx:1, w:tensor([0, 0]), b:-2\n",
      "idx:2, w:tensor([4, 3]), b:-1\n",
      "idx:1, w:tensor([3, 2]), b:-2\n",
      "idx:1, w:tensor([2, 1]), b:-3\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "count_ = 0\n",
    "while True:\n",
    "    if sign(model(sample[idx])) != lable[idx]:\n",
    "        w = w + lable[idx]*sample[idx]\n",
    "        b = b + lable[idx]\n",
    "\n",
    "        print(f\"idx:{idx}, w:{w}, b:{b}\")\n",
    "        count_ = 0\n",
    "    else:\n",
    "        idx += 1\n",
    "        idx %= 3\n",
    "        count_ += 1\n",
    "        if count_ > sample.shape[0]:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业4：\n",
    "* 4.1证明被线性激活函数激活的网络并不具备非线性表达能力(证明$y=\\theta x + b$成立)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{(3)}_1 = f(z^{(3)}_1)$\n",
    "\n",
    "$z^{(3)}_1 = \\theta^{(3)}_{11} a^{(2)}_1 + \\theta^{(3)}_{12} a^{(2)}_2 + \\theta^{(3)}_{13} a^{(2)}_3 + \\theta^{(3)}_{14} a^{(2)}_4 + b^2_1$\n",
    "\n",
    "$a^{(2)}_1 = f(z^{(2)}_1)$\n",
    "\n",
    "$a^{(2)}_2 = f(z^{(2)}_2)$\n",
    "\n",
    "$a^{(2)}_3 = f(z^{(2)}_3)$\n",
    "\n",
    "$a^{(2)}_4 = f(z^{(2)}_4)$\n",
    "\n",
    "$z^{(2)}_1 = \\theta^{(2)}_{11} x_1 + \\theta^{(2)}_{12} x_2 + \\theta^{(2)}_{13} x_3 + b^1_1$\n",
    "\n",
    "$z^{(2)}_2 = \\theta^{(2)}_{21} x_1 + \\theta^{(2)}_{22} x_2 + \\theta^{(2)}_{23} x_3 + b^1_2$\n",
    "\n",
    "$z^{(2)}_3 = \\theta^{(2)}_{31} x_1 + \\theta^{(2)}_{32} x_2 + \\theta^{(2)}_{33} x_3 + b^1_3$\n",
    "\n",
    "$z^{(2)}_4 = \\theta^{(2)}_{41} x_1 + \\theta^{(2)}_{42} x_2 + \\theta^{(2)}_{43} x_3 + b^1_4$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设$f(\\cdot)$激活函数为 $f(z^{(l)}_i) = z^{(l)}_i$\n",
    "\n",
    "则 $a^{(3)}_1 = \\theta^{(3)}_{11} a^{(2)}_1 + \\theta^{(3)}_{12} a^{(2)}_2 + \\theta^{(3)}_{13} a^{(2)}_3 + b^2_1$\n",
    "\n",
    "$a^{(3)}_1 = \\theta^{(3)}_{11} z^{(2)}_1 + \\theta^{(3)}_{12} z^{(2)}_2 + \\theta^{(3)}_{13} z^{(2)}_3 + b^2_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{(3)}_1 = \\theta^{(3)}_{11} (\\theta^{(2)}_{11} x_1 + \\theta^{(2)}_{12} x_2 + \\theta^{(2)}_{13} x_3 + b^1_1) + \\theta^{(3)}_{12} (\\theta^{(2)}_{21} x_1 + \\theta^{(2)}_{22} x_2 + \\theta^{(2)}_{23} x_3 + b^1_2) + \\theta^{(3)}_{13} (\\theta^{(2)}_{31} x_1 + \\theta^{(2)}_{32} x_2 + \\theta^{(2)}_{33} x_3 + b^1_3) + \\theta^{(3)}_{14} (\\theta^{(2)}_{41} x_1 + \\theta^{(2)}_{42} x_2 + \\theta^{(2)}_{43} x_3 + b^1_4) + b^2_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a^{(3)}_1 = \\theta^{(3)}_{11} (\\theta^{(2)}_{11} x_1 + \\theta^{(2)}_{12} x_2 + \\theta^{(2)}_{13} x_3 + b^1_1) + \\theta^{(3)}_{12} (\\theta^{(2)}_{21} x_1 + \\theta^{(2)}_{22} x_2 + \\theta^{(2)}_{23} x_3 + b^1_2) + \\theta^{(3)}_{13} (\\theta^{(2)}_{31} x_1 + \\theta^{(2)}_{32} x_2 + \\theta^{(2)}_{33} x_3 + b^1_3) + \\theta^{(3)}_{14} (\\theta^{(2)}_{41} x_1 + \\theta^{(2)}_{42} x_2 + \\theta^{(2)}_{43} x_3 + b^1_4) + b^2_1$\n",
    "\n",
    "$a^{(3)}_1 = x_1(\\theta^{(3)}_{11} \\theta^{(2)}_{11} + \\theta^{(3)}_{12} \\theta^{(2)}_{21} + \\theta^{(3)}_{13} \\theta^{(2)}_{31} + \\theta^{(3)}_{14} \\theta^{(2)}_{41})\n",
    "+\n",
    "x_2(\\theta^{(3)}_{11} \\theta^{(2)}_{12} + \\theta^{(3)}_{12} \\theta^{(2)}_{22} + \\theta^{(3)}_{13} \\theta^{(2)}_{32} + \\theta^{(3)}_{14} \\theta^{(2)}_{42})\n",
    "+\n",
    "x_3(\\theta^{(3)}_{11} \\theta^{(2)}_{13} + \\theta^{(3)}_{12} \\theta^{(2)}_{23} + \\theta^{(3)}_{13} \\theta^{(2)}_{33} + \\theta^{(3)}_{14} \\theta^{(2)}_{43})\n",
    "+\n",
    "\\theta^{(3)}_{11}b_{1}^1 + \\theta^{(3)}_{12}b_{2}^1 + \\theta^{(3)}_{13}b_{3}^1 +\\theta^{(3)}_{14}b^1_4 + b_1^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_1^{(3)} = \\Theta x + b$\n",
    "同理 a_2^{(3)}也可以整理成如上形式，可证明线性激活函数使神经网络不具备非线性表达能力"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 4.1使用神经网络时，我们通常需要将样本进行零均值处理，请从网络收敛速度（Z形更新）的角度思考为什么需要零均值化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\partial E \\over \\partial \\theta} = {\\partial E \\over \\partial a}{\\partial a \\over \\partial \\theta} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
