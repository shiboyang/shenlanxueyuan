{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='logo.png' align='center'>\n",
    "<div align='center' class=\"h2\">深度学习理论与实践编程练习(Course 03)</div>\n",
    "\n",
    "-----\n",
    "### 命名格式：按照课程网站中的课后作业要求\n",
    "-----\n",
    "\n",
    "#### 1. 根据Course03课程中对卷积神经网络的讲解，将缺失的全连接神经网络中代码块补全，并完成一次训练\n",
    "    [1] img2col 函数补全，通过补全函数，了解其实际含义\n",
    "    [2] Conv类中的前向过程\n",
    "    [3] Conv类中后向过程的 权重更新与误差反向传播\n",
    "    [4] Pool函数中的最大位置mask的计算过程\n",
    "    \n",
    "    \n",
    "#### 2. 修改卷积和池化的总操作数，观察对结果的影响\n",
    "    [1] 卷积后面跟随卷积层然后再接池化层，或者是用多个卷积池化操作的串联，观察对结果的影响。\n",
    "    [2] 修改的时候可以增加conv3和pool3实例，也可以修改卷积核的大小和卷积核的个数。\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='CNN.png' align='center'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 引入python包，没有安装的请按照抛出的error通过conda来安装直至成功\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(targets, num):\n",
    "    \"\"\"将数字的label转换成One-Hot的形式\"\"\"\n",
    "    result = np.zeros((num, 10))\n",
    "    for i in range(num):\n",
    "        result[i][targets[i]] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def img2col(x, ksize, step):\n",
    "    \"\"\"\n",
    "    将图像中所有需要卷积的地方转化成矩阵，方便卷积加速\n",
    "    :param x: 图像\n",
    "    :param ksize: 卷积大小\n",
    "    :param step: 步长\n",
    "    :return: 二维矩阵，每一行是所有深度上待卷积部分的一维形式\n",
    "    \"\"\"\n",
    "    # [width,height,channel] 宽，长，深度\n",
    "    wx, hx, cx = x.shape\n",
    "    # 返回的特征图尺寸\n",
    "    feature_w = (wx - ksize) // step + 1\n",
    "    # 图片矩阵的行就是图片能被卷积核匹配多少次的平方， 列就是卷积核reshap(-1)的平方再乘上深度\n",
    "    image_col = np.zeros((feature_w * feature_w, ksize * ksize * cx))\n",
    "    num = 0\n",
    "    ## 补全代码，补充image_col具体数值 ##\n",
    "    for i in range(feature_w):\n",
    "        for j in range(feature_w):\n",
    "            image_col[num] = x[i * step: i * step + ksize, j * step : j * step + ksize, :].reshape(-1)\n",
    "    return image_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Relu 函数\n",
    "class Relu(object):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def backward(self, delta):\n",
    "        delta[self.x < 0] = 0\n",
    "        return delta\n",
    "\n",
    "\n",
    "## Softmax 函数\n",
    "class Softmax(object):\n",
    "    def cal_loss(self, predict, label):\n",
    "        batchsize, classes = predict.shape\n",
    "        self.predict(predict)\n",
    "        loss = 0\n",
    "        delta = np.zeros(predict.shape)\n",
    "        for i in range(batchsize):\n",
    "            delta[i] = self.softmax[i] - label[i]\n",
    "            loss -= np.sum(np.log(self.softmax[i]) * label[i])\n",
    "        loss /= batchsize\n",
    "        return loss, delta\n",
    "\n",
    "    def predict(self, predict):\n",
    "        batchsize, classes = predict.shape\n",
    "        self.softmax = np.zeros(predict.shape)\n",
    "        for i in range(batchsize):\n",
    "            predict_tmp = predict[i] - np.max(predict[i])\n",
    "            predict_tmp = np.exp(predict_tmp)\n",
    "            self.softmax[i] = predict_tmp / np.sum(predict_tmp)\n",
    "        return self.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./datasets/mnist\"\n",
    "train_data = torchvision.datasets.MNIST(root=dataset_path, train=True, download=False)\n",
    "train_data.data = train_data.data.numpy()  # [60000,28,28]\n",
    "train_data.targets = train_data.targets.numpy()  # [60000]\n",
    "train_data.data = train_data.data.reshape(60000, 28, 28, 1) / 255.  # 输入向量处理\n",
    "train_data.targets = onehot(train_data.targets, 60000)  # 标签one-hot处理 (60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 全连接层\n",
    "class Linear(object):\n",
    "\n",
    "    def __init__(self, inChannel, outChannel):\n",
    "        scale = np.sqrt(inChannel / 2)\n",
    "        self.W = np.random.standard_normal((inChannel, outChannel)) / scale\n",
    "        self.b = np.random.standard_normal(outChannel) / scale\n",
    "        self.W_gradient = np.zeros((inChannel, outChannel))\n",
    "        self.b_gradient = np.zeros(outChannel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"前向过程\"\"\"\n",
    "        ## 补全代码 ##\n",
    "        self.x = x \n",
    "        return x @ self.W + self.b\n",
    "\n",
    "    def backward(self, delta, learning_rate):\n",
    "        \"\"\"反向过程\"\"\"\n",
    "        ## 梯度计算\n",
    "        batch_size = self.x.shape[0]\n",
    "        \n",
    "        ## 补全代码 ##\n",
    "        self.W_gradient = delta @ self.x.transport()\n",
    "        self.b_gradient = delta\n",
    "        delta_backward = delta * \n",
    "        ## 反向传播\n",
    "        self.W -= learning_rate * self.W_gradient\n",
    "        self.b -= learning_rate * self.b_gradient\n",
    "\n",
    "        return delta_backward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## conv\n",
    "class Conv(object):\n",
    "    def __init__(self, kernel_shape, step=1, pad=0):\n",
    "        # [w, h, d]\n",
    "        width, height, in_channel, out_channel = kernel_shape\n",
    "        self.step = step\n",
    "        self.pad = pad\n",
    "        scale = np.sqrt(3 * in_channel * width * height / out_channel)\n",
    "        self.k = np.random.standard_normal(kernel_shape) / scale\n",
    "        self.b = np.random.standard_normal(out_channel) / scale\n",
    "        self.k_gradient = np.zeros(kernel_shape)\n",
    "        self.b_gradient = np.zeros(out_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        if self.pad != 0:\n",
    "            self.x = np.pad(self.x, ((0, 0), (self.pad, self.pad), (self.pad, self.pad), (0, 0)), 'constant')\n",
    "        bx, wx, hx, cx = self.x.shape\n",
    "        # kernel的宽、高、通道数、个数\n",
    "        wk, hk, ck, nk = self.k.shape\n",
    "        feature_w = (wx - wk) // self.step + 1  # 返回的特征图尺寸\n",
    "        feature = np.zeros((bx, feature_w, feature_w, nk))\n",
    "\n",
    "        self.image_col = []\n",
    "        # kernal也进行了reshape，便于卷积加速，只保留通道维度，是个二维的矩阵\n",
    "        kernel = self.k.reshape(-1, nk)\n",
    "        \n",
    "        ## 补全代码 ##\n",
    "        for i in range(bx):\n",
    "            image_col = \n",
    "            feature[i] = \n",
    "            self.image_col.append()\n",
    "        return feature\n",
    "\n",
    "    def backward(self, delta, learning_rate):\n",
    "        bx, wx, hx, cx = self.x.shape  # batch,14,14,inchannel\n",
    "        wk, hk, ck, nk = self.k.shape  # 5,5,inChannel,outChannel\n",
    "        bd, wd, hd, cd = delta.shape   # batch,10,10,outChannel\n",
    "\n",
    "        # 计算self.k_gradient,self.b_gradient\n",
    "        # 参数更新过程\n",
    "        ## 补全代码 ##\n",
    "        delta_col = delta.reshape(bd, -1, cd)\n",
    "        for i in range(bx):\n",
    "            self.k_gradient += \n",
    "        self.k_gradient /= \n",
    "        self.b_gradient += \n",
    "        self.b_gradient /= \n",
    "\n",
    "        # 计算delta_backward\n",
    "        # 误差的反向传递\n",
    "        delta_backward = np.zeros(self.x.shape)\n",
    "        # numpy矩阵（对应kernal）旋转180度\n",
    "        ## 补全代码 ##\n",
    "        k_180 = \n",
    "        k_180 = \n",
    "        k_180_col = \n",
    "\n",
    "        if hd - hk + 1 != hx:\n",
    "            pad = (hx - hd + hk - 1) // 2\n",
    "            pad_delta = np.pad(delta, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant')\n",
    "        else:\n",
    "            pad_delta = delta\n",
    "\n",
    "        for i in range(bx):\n",
    "            pad_delta_col = img2col(pad_delta[i], wk, self.step)\n",
    "            delta_backward[i] = np.dot(pad_delta_col, k_180_col).reshape(wx, hx, ck)\n",
    "\n",
    "        # 反向传播\n",
    "        self.k -= self.k_gradient * learning_rate\n",
    "        self.b -= self.b_gradient * learning_rate\n",
    "\n",
    "        return delta_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Max Pooling层\n",
    "class Pool(object):\n",
    "    def forward(self, x):\n",
    "        b, w, h, c = x.shape\n",
    "        feature_w = w // 2\n",
    "        feature = np.zeros((b, feature_w, feature_w, c))\n",
    "        self.feature_mask = np.zeros((b, w, h, c))  # 记录最大池化时最大值的位置信息用于反向传播\n",
    "        for bi in range(b):\n",
    "            for ci in range(c):\n",
    "                for i in range(feature_w):\n",
    "                    for j in range(feature_w):\n",
    "                        ## 补全代码\n",
    "                        feature[bi, i, j, ci] = \n",
    "                        index = np.argmax(x[bi, i * 2:i * 2 + 2, j * 2:j * 2 + 2, ci])\n",
    "                        self.feature_mask[bi, i * 2 + index // 2, j * 2 + index % 2, ci] = 1\n",
    "        return feature\n",
    "\n",
    "    def backward(self, delta):\n",
    "        return np.repeat(np.repeat(delta, 2, axis=1), 2, axis=2) * self.feature_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(batch=32, lr=0.01, epochs=10):\n",
    "    # Mnist手写数字集\n",
    "    dataset_path = \"./datasets/mnist\"\n",
    "    train_data = torchvision.datasets.MNIST(root=dataset_path, train=True, download=False)\n",
    "    train_data.data = train_data.data.numpy()  # [60000,28,28]\n",
    "    train_data.targets = train_data.targets.numpy()  # [60000]\n",
    "    train_data.data = train_data.data.reshape(60000, 28, 28, 1) / 255.  # 输入向量处理\n",
    "    train_data.targets = onehot(train_data.targets, 60000)  # 标签one-hot处理 (60000, 10)\n",
    "    \n",
    "    # [28,28] 卷积 6x[5,5] -> 6x[24,24]\n",
    "    conv1 = Conv(kernel_shape=(5, 5, 1, 6))\n",
    "    relu1 = Relu()\n",
    "    # 6x[24,24] -> 6x[12,12]\n",
    "    pool1 = Pool() \n",
    "    # 6x[12,12] 卷积 16x(6x[12,12]) -> 16x[8,8]\n",
    "    conv2 = Conv(kernel_shape=(5, 5, 6, 16))  # 8x8x16\n",
    "    relu2 = Relu()\n",
    "    # 16x[8,8] -> 16x[4,4]\n",
    "    pool2 = Pool() \n",
    "    \n",
    "    # 在这里可以尝试增加网络的深度，再实例化conv3和pool3，记得后面的前向传播过程\n",
    "    # 和反向传播过程也要有对应的过程\n",
    "    \n",
    "    nn = Linear(256, 10)\n",
    "    softmax = Softmax()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, 60000, batch):\n",
    "            X = train_data.data[i:i + batch]\n",
    "            Y = train_data.targets[i:i + batch]\n",
    "\n",
    "            # 前向传播过程\n",
    "            predict = conv1.forward(X)\n",
    "            predict = relu1.forward(predict)\n",
    "            predict = pool1.forward(predict)\n",
    "            predict = conv2.forward(predict)\n",
    "            predict = relu2.forward(predict)\n",
    "            predict = pool2.forward(predict)\n",
    "            predict = predict.reshape(batch, -1)\n",
    "            predict = nn.forward(predict)\n",
    "\n",
    "            # 误差计算\n",
    "            loss, delta = softmax.cal_loss(predict, Y)\n",
    "            \n",
    "            # 反向传播过程\n",
    "            delta = nn.backward(delta, lr)\n",
    "            delta = delta.reshape(batch, 4, 4, 16)\n",
    "            delta = pool2.backward(delta)\n",
    "            delta = relu2.backward(delta)\n",
    "            delta = conv2.backward(delta, lr)\n",
    "            delta = pool1.backward(delta)\n",
    "            delta = relu1.backward(delta)\n",
    "            conv1.backward(delta, lr)\n",
    "\n",
    "            print(\"Epoch-{}-{:05d}\".format(str(epoch), i), \":\", \"loss:{:.4f}\".format(loss))\n",
    "\n",
    "        lr *= 0.95 ** (epoch + 1)\n",
    "        np.savez(\"simple_cnn_model.npz\", k1=conv1.k, b1=conv1.b, k2=conv2.k, b2=conv2.b, w3=nn.W, b3=nn.b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval():\n",
    "    model = np.load(\"simple_cnn_model.npz\")\n",
    "\n",
    "    dataset_path = \"./datasets/mnist\"\n",
    "    test_data = torchvision.datasets.MNIST(root=dataset_path, train=False)\n",
    "    test_data.data = test_data.data.numpy()  # [10000,28,28]\n",
    "    test_data.targets = test_data.targets.numpy()  # [10000]\n",
    "\n",
    "    test_data.data = test_data.data.reshape(10000, 28, 28, 1) / 255.\n",
    "\n",
    "    conv1 = Conv(kernel_shape=(5, 5, 1, 6))  # 24x24x6\n",
    "    relu1 = Relu()\n",
    "    pool1 = Pool()  # 12x12x6\n",
    "    conv2 = Conv(kernel_shape=(5, 5, 6, 16))  # 8x8x16\n",
    "    relu2 = Relu()\n",
    "    pool2 = Pool()  # 4x4x16\n",
    "    nn = Linear(256, 10)\n",
    "    softmax = Softmax()\n",
    "\n",
    "    conv1.k = model[\"k1\"]\n",
    "    conv1.b = model[\"b1\"]\n",
    "    conv2.k = model[\"k2\"]\n",
    "    conv2.b = model[\"b2\"]\n",
    "    nn.W = model[\"w3\"]\n",
    "    nn.n = model[\"b3\"]\n",
    "\n",
    "    num = 0\n",
    "    for i in range(10000):\n",
    "        X = test_data.data[i]\n",
    "        X = X[np.newaxis, :]\n",
    "        Y = test_data.targets[i]\n",
    "\n",
    "        predict = conv1.forward(X)\n",
    "        predict = relu1.forward(predict)\n",
    "        predict = pool1.forward(predict)\n",
    "        predict = conv2.forward(predict)\n",
    "        predict = relu2.forward(predict)\n",
    "        predict = pool2.forward(predict)\n",
    "        predict = predict.reshape(1, -1)\n",
    "        predict = nn.forward(predict)\n",
    "\n",
    "        predict = softmax.predict(predict)\n",
    "\n",
    "        if np.argmax(predict) == Y:\n",
    "            num += 1\n",
    "\n",
    "    print(\"TEST-ACC: \", num / 10000 * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
